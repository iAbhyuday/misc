{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import gradio\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b66f91b96c4628981c6dd1ad3e8e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "pipeline = AutoPipelineForInpainting.from_pretrained(\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd4a44ca092456c8b18ffdecebb5283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_mask.png\")\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_1.png\")\n",
    "ip_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_gummy.png\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(4)\n",
    "images = pipeline(\n",
    "    prompt=\"a cute gummy bear waving\",\n",
    "    image=image,\n",
    "    mask_image=mask_image,\n",
    "    ip_adapter_image=ip_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=100,\n",
    ").images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = gradio.ImageMask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/gradio/route_utils.py\", line 233, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1608, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/gradio/utils.py\", line 689, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2512644/3882120017.py\", line 35, in sleep\n",
      "    images = pipeline(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py\", line 1543, in __call__\n",
      "    latents_outputs = self.prepare_latents(\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py\", line 907, in prepare_latents\n",
      "    latents = noise if is_strength_max else self.scheduler.add_noise(image_latents, noise, timestep)\n",
      "  File \"/home/z004e29c/envs/pytorch/lib/python3.10/site-packages/diffusers/schedulers/scheduling_euler_discrete.py\", line 572, in add_noise\n",
      "    noisy_samples = original_samples + noise * sigma\n",
      "RuntimeError: The size of tensor a (1024) must match the size of tensor b (128) at non-singleton dimension 3\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "from PIL import Image\n",
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "\n",
    "\n",
    "output_height = 1024\n",
    "output_width = 1024\n",
    "\n",
    "processor = IPAdapterMaskProcessor()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    im = gr.ImageMask(\n",
    "        type=\"pil\",\n",
    "        crop_size=\"1:1\",\n",
    "    )\n",
    "\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            im_out_1 = gr.Image(type=\"pil\")\n",
    "            im_out_2 = gr.Image(type=\"pil\")\n",
    "            im_out_3 = gr.Image(type=\"pil\")\n",
    "\n",
    "    btn = gr.Button(\"sleep\")\n",
    "    @btn.click(inputs=im, outputs=[im_out_1, im_out_2, im_out_3])\n",
    "    def sleep(im):\n",
    "        #time.sleep(5)\n",
    "        \n",
    "        mask = im[\"layers\"][0]\n",
    "        mask = processor.preprocess([mask], height=output_height, width=output_width)[0]\n",
    "        print(np.asarray(mask).shape)\n",
    "        image = im[\"background\"]\n",
    "        ip_image = load_image(\"/home/z004e29c/Pictures/Screenshots/logo.png\")\n",
    "        generator = torch.Generator(device=\"cpu\").manual_seed(4)\n",
    "        images = pipeline(\n",
    "            prompt=\"a cute gummy bear waving\",\n",
    "            image=image,\n",
    "            mask_image=mask_image,\n",
    "            ip_adapter_image=ip_image,\n",
    "            generator=generator,\n",
    "            num_inference_steps=100,\n",
    "        ).images\n",
    "        \n",
    "\n",
    "        return [im[\"background\"], im[\"layers\"][0], images[0]]\n",
    "\n",
    "    \n",
    "    #im.change(sleep, outputs=[im_out_1, im_out_2, im_out_3, im_out_4], inputs=im)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/z004e29c/projects/FY24/cmc/diffusion/ip-adapter.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/z004e29c/projects/FY24/cmc/diffusion/ip-adapter.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39masarray(mask)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "np.asarray(mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
