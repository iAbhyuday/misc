{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import ipywidgets\n",
    "from IPython.display import display, Image as pyImage, HTML\n",
    "import numpy as np\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.models import UNet2DModel\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST,CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop,RandomHorizontalFlip, PILToTensor\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def create_gif(images, output_filename, duration=0.2):\n",
    "    writer = None\n",
    "    if images.shape[-1]==1:\n",
    "        images = np.repeat(images, 3, axis=-1)\n",
    "    try:\n",
    "        writer = imageio.get_writer(output_filename, mode='I', duration=duration)\n",
    "        for img in images:\n",
    "            writer.append_data(img)\n",
    "        yield writer\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    RandomCrop(32),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = CIFAR10(\"data\", train=True, download=False, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tr_data, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(x):\n",
    "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
    "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
    "    x = x.cpu().permute(0, 2, 3, 1).clip(0, 1).numpy()*255\n",
    "    x = x.astype(np.uint8)\n",
    "    gif_path = \"data/sample.gif\"\n",
    "    with create_gif(x, gif_path, 0.5) as gif_writer:\n",
    "        gif_writer.close()\n",
    "    return display(pyImage(filename=gif_path, width=1280, height=128))\n",
    "    \n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = img * 0.5 + 0.5\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img,dtype=np.uint8))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "grid = make_grid(x,)\n",
    "show((grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=(32, 32),  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(params=model.parameters(), lr=4e-4)\n",
    "sched = DDPMScheduler(num_train_timesteps=50, beta_schedule=\"squaredcos_cap_v2\")\n",
    "mse = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(30):\n",
    "    running_loss=0.\n",
    "    with tqdm(train_loader) as mb_train:\n",
    "        for x, y in mb_train:\n",
    "            x = x.to(device)\n",
    "            bs = x.shape[0]\n",
    "            noise = torch.randn_like(x).to(device)\n",
    "            timesteps = torch.randint(high=50, size=(bs,)).to(device)\n",
    "            noisy_x = sched.add_noise(x, noise, timesteps=timesteps)\n",
    "            eps = model(noisy_x, timesteps)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = mse(eps.sample, noise)\n",
    "            running_loss+=loss.mean().item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            mb_train.set_postfix({\"loss\": loss.mean().item()})\n",
    "    print(f\"Epoch {e+1} loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn((1, 1, 32, 32)).to(device)\n",
    "        hist = []\n",
    "        for t in sched.timesteps:\n",
    "            res = model(sample, t).sample\n",
    "            sample = sched.step(res, t, sample).prev_sample\n",
    "            \n",
    "            hist.append(sample)\n",
    "        hist = torch.concat(hist, dim=0)\n",
    "        show_images(hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class guided generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DModelCC(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, in_channels, n_embed):\n",
    "        super(UNet2DModelCC, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_classes, n_embed)\n",
    "        self.unet = UNet2DModel(\n",
    "                    sample_size=(32, 32),  # the target image resolution\n",
    "                    in_channels=in_channels + n_embed,  # the number of input channels, 3 for RGB images\n",
    "                    out_channels=in_channels,  # the number of output channels\n",
    "                    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "                    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "                    down_block_types=(\n",
    "                        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "                        \"DownBlock2D\",\n",
    "                        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "                        \"AttnDownBlock2D\",\n",
    "                    ),\n",
    "                    up_block_types=(\n",
    "                        \"AttnUpBlock2D\",\n",
    "                        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "                        \"UpBlock2D\",\n",
    "                        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "                    ),\n",
    "                    )\n",
    "    \n",
    "    def forward(self, x, timestep, y):\n",
    "        bs = x.shape[0]\n",
    "        class_embed = self.embedding(y)\n",
    "        class_embed = class_embed.view(bs, class_embed.shape[-1], 1, 1).expand(bs, class_embed.shape[-1], x.shape[2], x.shape[3])\n",
    "        x = torch.cat([x, class_embed], dim=1)\n",
    "        return self.unet(x, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModelCC(10, 3, 8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn((32, 3, 32, 32)).to(device)\n",
    "    y = torch.randint(10, (32,1)).to(device)\n",
    "    timesteps = torch.randint(50, (32,)).to(device)\n",
    "    out = model(x, timesteps, y).sample\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(params=model.parameters(), lr=4e-4)\n",
    "sched = DDPMScheduler(num_train_timesteps=100, beta_schedule=\"squaredcos_cap_v2\")\n",
    "mse = MSELoss()\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(30):\n",
    "    running_loss=0.\n",
    "    with tqdm(train_loader) as mb_train:\n",
    "        for mb_idx, batch in enumerate(mb_train):\n",
    "            x,y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            bs = x.shape[0]\n",
    "            noise = torch.randn_like(x).to(device)\n",
    "            timesteps = torch.randint(high=100, size=(bs,)).to(device)\n",
    "            noisy_x = sched.add_noise(x, noise, timesteps=timesteps)\n",
    "            eps = model(noisy_x, timesteps, y).sample\n",
    "            loss = mse(eps, noise)\n",
    "            running_loss+=loss.mean().item()\n",
    "            loss.backward()\n",
    "            \n",
    "            if mb_idx%2==0:\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "\n",
    "            mb_train.set_postfix({\"loss\": loss.mean().item()})\n",
    "            \n",
    "    print(f\"Epoch {e+1} loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn((10, 3, 32, 32)).to(device)\n",
    "        hist = []\n",
    "        y_ = torch.arange(10, device=device)\n",
    "        for t in sched.timesteps:\n",
    "            res = model(sample, t, y_).sample\n",
    "            sample = sched.step(res, t, sample).prev_sample\n",
    "            if t%10==0:\n",
    "                hist.append(make_grid(sample,nrow=10))\n",
    "        hist = torch.stack(hist, dim=0)\n",
    "        show_images(hist)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
